# Edge-Deployable AI Planner

**ğŸ“Œ Purpose of This Branch**

This branch serves as a separate environment to add core functionality missing in `main` (post-project submission):

- ğŸ”§ Fixing the broken CI/CD pipeline  
- âœ… Adding unit tests for core functionality  
- ğŸ§¹ Refactoring the codebase for improved modularity  
- ğŸ³ Building and containerising this application with Docker  
- ğŸš€ Integrating deployment via MLC-LLM  
- ğŸ“Š Extending and automating benchmarking processes  
- ğŸ§  Enhancing the agentic workflow by adding a routing step post-input prompt  

These changes aim to create a cleaner, more maintainable, and production-ready codebase.

---

## ğŸ³ Run the Planner from Docker Hub

You can run the quantised AI planner agent directly using our pre-built Docker image. This image simulates a local, edge-deployable AI system using the [Mistral 7B Instruct (Q3_K_M)](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q3_K_M.gguf) model, optimised for CPU inference via `llama.cpp`.

### ğŸ”§ Requirements

- Docker installed on your machine
- At least ~4GB free disk space
- macOS (Apple Silicon) or Linux recommended

---

### ğŸ“¦ Step 1: Pull the Container

```bash
docker pull michaelsigamani/agentic-planner-8b:latest
```

---

### â–¶ï¸ Step 2: Run Calendar Scheduling Benchmarks

```bash
docker run --rm -it michaelsigamani/agentic-planner-8b:latest python run_cal_benchmarks.py
```

This command will:

- Run the **Calendar Scheduling** benchmark from Google DeepMindâ€™s [NaturalPlan](https://arxiv.org/pdf/2406.04520) dataset
- Use **in-context learning** with a **4-bit quantised model**
- Log evaluation metrics to **Weights & Biases** and traces to **LangSmith**

---

### ğŸ” What's Next?

You can:

- Modify `run_cal_benchmarks.py` to target other subtasks like trip planning
- Mount your own `gguf` models to `/models` and adjust the `LlamaCpp` config
- Extend LangGraph logic for smarter agent routing and retries
- Integrate with `mlc-llm` for mobile or GPU-accelerated inference

---
